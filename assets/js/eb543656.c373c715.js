"use strict";(self.webpackChunkathenaserving=self.webpackChunkathenaserving||[]).push([[8412],{3905:(e,n,a)=>{a.d(n,{Zo:()=>u,kt:()=>h});var r=a(67294);function t(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function o(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,r)}return a}function i(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?o(Object(a),!0).forEach((function(n){t(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function p(e,n){if(null==e)return{};var a,r,t=function(e,n){if(null==e)return{};var a,r,t={},o=Object.keys(e);for(r=0;r<o.length;r++)a=o[r],n.indexOf(a)>=0||(t[a]=e[a]);return t}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)a=o[r],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(t[a]=e[a])}return t}var l=r.createContext({}),s=function(e){var n=r.useContext(l),a=n;return e&&(a="function"==typeof e?e(n):i(i({},n),e)),a},u=function(e){var n=s(e.components);return r.createElement(l.Provider,{value:n},e.children)},d="mdxType",c={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},m=r.forwardRef((function(e,n){var a=e.components,t=e.mdxType,o=e.originalType,l=e.parentName,u=p(e,["components","mdxType","originalType","parentName"]),d=s(a),m=t,h=d["".concat(l,".").concat(m)]||d[m]||c[m]||o;return a?r.createElement(h,i(i({ref:n},u),{},{components:a})):r.createElement(h,i({ref:n},u))}));function h(e,n){var a=arguments,t=n&&n.mdxType;if("string"==typeof e||t){var o=a.length,i=new Array(o);i[0]=m;var p={};for(var l in n)hasOwnProperty.call(n,l)&&(p[l]=n[l]);p.originalType=e,p[d]="string"==typeof e?e:t,i[1]=p;for(var s=2;s<o;s++)i[s]=a[s];return r.createElement.apply(null,i)}return r.createElement.apply(null,a)}m.displayName="MDXCreateElement"},20451:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>c,frontMatter:()=>o,metadata:()=>p,toc:()=>s});var r=a(87462),t=(a(67294),a(3905));const o={sidebar_position:3,sidebar_label:"\u5b89\u88c5\u9ad8\u53ef\u7528"},i=void 0,p={unversionedId:"\u6570\u636e\u5b58\u50a8/Apache Hadoop/\u5b89\u88c5\u9ad8\u53ef\u7528",id:"\u6570\u636e\u5b58\u50a8/Apache Hadoop/\u5b89\u88c5\u9ad8\u53ef\u7528",title:"\u5b89\u88c5\u9ad8\u53ef\u7528",description:"\u6982\u8ff0",source:"@site/docs/\u6570\u636e\u5b58\u50a8/Apache Hadoop/\u5b89\u88c5\u9ad8\u53ef\u7528.md",sourceDirName:"\u6570\u636e\u5b58\u50a8/Apache Hadoop",slug:"/\u6570\u636e\u5b58\u50a8/Apache Hadoop/\u5b89\u88c5\u9ad8\u53ef\u7528",permalink:"/docs/\u6570\u636e\u5b58\u50a8/Apache Hadoop/\u5b89\u88c5\u9ad8\u53ef\u7528",draft:!1,editUrl:"https://github.com/zhangyongtian/bigdataknowledge/tree/dev/docs/\u6570\u636e\u5b58\u50a8/Apache Hadoop/\u5b89\u88c5\u9ad8\u53ef\u7528.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,sidebar_label:"\u5b89\u88c5\u9ad8\u53ef\u7528"},sidebar:"docSidebar",previous:{title:"\u5b89\u88c5\u73af\u5883\u51c6\u5907",permalink:"/docs/\u6570\u636e\u5b58\u50a8/Apache Hadoop/\u5b89\u88c5\u73af\u5883\u51c6\u5907"},next:{title:"Apache Hive",permalink:"/docs/category/apache-hive"}},l={},s=[{value:"\u6982\u8ff0",id:"\u6982\u8ff0",level:2},{value:"\u5b89\u88c5\u6b65\u9aa4",id:"\u5b89\u88c5\u6b65\u9aa4",level:2},{value:"\u89e3\u538b\u5b89\u88c5\u5305",id:"\u89e3\u538b\u5b89\u88c5\u5305",level:3},{value:"\u914d\u7f6e\u73af\u5883\u53d8\u91cf",id:"\u914d\u7f6e\u73af\u5883\u53d8\u91cf",level:3},{value:"Hadoop \u76f8\u5173\u6587\u4ef6\u8bf4\u660e",id:"hadoop-\u76f8\u5173\u6587\u4ef6\u8bf4\u660e",level:2},{value:"\u4fee\u6539\u914d\u7f6e\u6587\u4ef6",id:"\u4fee\u6539\u914d\u7f6e\u6587\u4ef6",level:2},{value:"workers",id:"workers",level:3},{value:"core-site.xml",id:"core-sitexml",level:3},{value:"hdfs-site.xml",id:"hdfs-sitexml",level:3},{value:"yarn-site.xml",id:"yarn-sitexml",level:3},{value:"mapred-site.xml",id:"mapred-sitexml",level:3},{value:"capacity-scheduler.xml",id:"capacity-schedulerxml",level:3},{value:"hadoop-env.sh",id:"hadoop-envsh",level:3},{value:"\u540c\u6b65\u914d\u7f6e\u6587\u4ef6",id:"\u540c\u6b65\u914d\u7f6e\u6587\u4ef6",level:3},{value:"\u542f\u52a8\u96c6\u7fa4",id:"\u542f\u52a8\u96c6\u7fa4",level:2},{value:"\u542f\u52a8\u51c6\u5907",id:"\u542f\u52a8\u51c6\u5907",level:3},{value:"\u96c6\u7fa4\u542f\u52a8\u811a\u672c",id:"\u96c6\u7fa4\u542f\u52a8\u811a\u672c",level:3},{value:"\u6d4b\u8bd5\u770b\u662f\u5426\u542f\u52a8\u6210\u529f",id:"\u6d4b\u8bd5\u770b\u662f\u5426\u542f\u52a8\u6210\u529f",level:2},{value:"\u67e5\u770bhdfs\u7684\u9ad8\u53ef\u7528\u72b6\u6001",id:"\u67e5\u770bhdfs\u7684\u9ad8\u53ef\u7528\u72b6\u6001",level:3},{value:"\u63d0\u4ea4\u4e00\u4e2a mr \u7a0b\u5e8f\u3002",id:"\u63d0\u4ea4\u4e00\u4e2a-mr-\u7a0b\u5e8f",level:3}],u={toc:s},d="wrapper";function c(e){let{components:n,...a}=e;return(0,t.kt)(d,(0,r.Z)({},u,a,{components:n,mdxType:"MDXLayout"}),(0,t.kt)("h2",{id:"\u6982\u8ff0"},"\u6982\u8ff0"),(0,t.kt)("h2",{id:"\u5b89\u88c5\u6b65\u9aa4"},"\u5b89\u88c5\u6b65\u9aa4"),(0,t.kt)("h3",{id:"\u89e3\u538b\u5b89\u88c5\u5305"},"\u89e3\u538b\u5b89\u88c5\u5305"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"tar -zxvf hadoop-3.2.3.tar.gz -C ../module/\n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"\x3c!-- \u540c\u6b65\u5b89\u88c5\u5305 --\x3e\n\n/home/bigdata/shell/xsync.sh /home/bigdata/module/hadoop-3.2.3\n")),(0,t.kt)("h3",{id:"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"sudo vim /etc/profile.d/my_env.sh\n\n#HADOOP_HOME\n\nexport HADOOP_HOME=/home/bigdata/module/hadoop-3.2.3\nexport PATH=$PATH:$HADOOP_HOME/bin\nexport PATH=$PATH:$HADOOP_HOME/sbin\n\n")),(0,t.kt)("p",null,"\u540c\u6b65\u73af\u5883\u53d8\u91cf\u6587\u4ef6\u3002"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"/home/bigdata/shell/xsync.sh /etc/profile.d/my_env.sh\n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"source /etc/profile.d/my_env.sh\n")),(0,t.kt)("h2",{id:"hadoop-\u76f8\u5173\u6587\u4ef6\u8bf4\u660e"},"Hadoop \u76f8\u5173\u6587\u4ef6\u8bf4\u660e"),(0,t.kt)("ul",null,(0,t.kt)("li",{parentName:"ul"},(0,t.kt)("p",{parentName:"li"},"bin\u76ee\u5f55\uff1a\u5b58\u653e\u5bf9Hadoop\u76f8\u5173\u670d\u52a1\uff08hdfs\uff0cyarn\uff0cmapred\uff09\u8fdb\u884c\u64cd\u4f5c\u7684\u811a\u672c")),(0,t.kt)("li",{parentName:"ul"},(0,t.kt)("p",{parentName:"li"},"etc\u76ee\u5f55\uff1aHadoop\u7684\u914d\u7f6e\u6587\u4ef6\u76ee\u5f55\uff0c\u5b58\u653eHadoop\u7684\u914d\u7f6e\u6587\u4ef6")),(0,t.kt)("li",{parentName:"ul"},(0,t.kt)("p",{parentName:"li"},"lib\u76ee\u5f55\uff1a\u5b58\u653eHadoop\u7684\u672c\u5730\u5e93\uff08\u5bf9\u6570\u636e\u8fdb\u884c\u538b\u7f29\u89e3\u538b\u7f29\u529f\u80fd\uff09")),(0,t.kt)("li",{parentName:"ul"},(0,t.kt)("p",{parentName:"li"},"sbin\u76ee\u5f55\uff1a\u5b58\u653e\u542f\u52a8\u6216\u505c\u6b62Hadoop\u76f8\u5173\u670d\u52a1\u7684\u811a\u672c")),(0,t.kt)("li",{parentName:"ul"},(0,t.kt)("p",{parentName:"li"},"share\u76ee\u5f55\uff1a\u5b58\u653eHadoop\u7684\u4f9d\u8d56jar\u5305\u3001\u6587\u6863\u3001\u548c\u5b98\u65b9\u6848\u4f8b"))),(0,t.kt)("h2",{id:"\u4fee\u6539\u914d\u7f6e\u6587\u4ef6"},"\u4fee\u6539\u914d\u7f6e\u6587\u4ef6"),(0,t.kt)("h3",{id:"workers"},"workers"),(0,t.kt)("p",null,"workers \u6587\u4ef6\u914d\u7f6e\u662f\u5728\u96c6\u7fa4\u811a\u672c\u542f\u52a8datanode\u548cnodemanage\u5305\u542b\u7684\u673a\u5668\uff0c",(0,t.kt)("strong",{parentName:"p"},"\u4e0d\u5305\u542bnamenode\u548cresourcemanager"),"\u3002"),(0,t.kt)("p",null,"\u7531\u4e8e\u89c4\u5212"),(0,t.kt)("table",null,(0,t.kt)("thead",{parentName:"table"},(0,t.kt)("tr",{parentName:"thead"},(0,t.kt)("th",{parentName:"tr",align:null},"\u4e3b\u673a"),(0,t.kt)("th",{parentName:"tr",align:null},"\u89c4\u5212\u8bbe\u7f6e\u4e3b\u673a\u540d"),(0,t.kt)("th",{parentName:"tr",align:null},"\u89d2\u8272"))),(0,t.kt)("tbody",{parentName:"table"},(0,t.kt)("tr",{parentName:"tbody"},(0,t.kt)("td",{parentName:"tr",align:null},"10.240.8.68"),(0,t.kt)("td",{parentName:"tr",align:null},"master1"),(0,t.kt)("td",{parentName:"tr",align:null},"NameNode\u3001DataNode\u3001ResourceManager\u3001NodeManager")),(0,t.kt)("tr",{parentName:"tbody"},(0,t.kt)("td",{parentName:"tr",align:null},"10.240.8.229"),(0,t.kt)("td",{parentName:"tr",align:null},"master2"),(0,t.kt)("td",{parentName:"tr",align:null},"SecondaryNameNode\u3001DataNode\u3001NodeManager")),(0,t.kt)("tr",{parentName:"tbody"},(0,t.kt)("td",{parentName:"tr",align:null},"10.240.8.185"),(0,t.kt)("td",{parentName:"tr",align:null},"node1"),(0,t.kt)("td",{parentName:"tr",align:null},"DataNode\u3001NodeManager")))),(0,t.kt)("p",null,"master1\uff0cmaster2\uff0cnode1 \u90fd\u6709DataNode\u3001NodeManager\uff0c\u6240\u4ee5\u4fee\u6539\u5982\u4e0b\u3002"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"vi /home/bigdata/module/hadoop-3.2.3/etc/hadoop/workers\n\nmaster1\nmaster2\nnode1\n")),(0,t.kt)("h3",{id:"core-sitexml"},"core-site.xml"),(0,t.kt)("p",null,"\u51c6\u5907\u597d\u6570\u636e\u76d8,\u7136\u540e\u628a\u6570\u636e\u76d8\u7684\u6240\u6709\u6743\u7ed9bigdata"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"sudo chown bigdata:bigdata -R /datadrive/\nsudo chmod 744 /datadrive\n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"vi /home/bigdata/module/hadoop-3.2.3/etc/hadoop/core-site.xml\n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},'<?xml version="1.0" encoding="UTF-8"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n\x3c!--\n  Licensed under the Apache License, Version 2.0 (the "License");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an "AS IS" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n--\x3e\n\n\x3c!-- Put site-specific property overrides in this file. --\x3e\n\n<configuration>\n\n    <property>\n        \x3c!--\u6307\u5b9a namenode \u7684 hdfs \u534f\u8bae\u6587\u4ef6\u7cfb\u7edf\u7684\u901a\u4fe1\u5730\u5740--\x3e\n        <name>fs.defaultFS</name>\n        \x3c!--\u6307\u5b9ahdfs\u9ad8\u53ef\u7528\u7684\u96c6\u7fa4\u540d\u79f0--\x3e\n        <value>hdfs://bigdatacluster</value>\n    </property>\n    <property>\n        \x3c!--\u6307\u5b9a hadoop \u96c6\u7fa4\u5b58\u50a8\u4e34\u65f6\u6587\u4ef6\u7684\u76ee\u5f55--\x3e\n        <name>hadoop.tmp.dir</name>\n        <value>/datadrive</value>\n    </property>\n    \n    \x3c!-- \u914d\u7f6eHDFS\u7f51\u9875\u767b\u5f55\u4f7f\u7528\u7684\u9759\u6001\u7528\u6237\u4e3abigdata --\x3e\n  <property>\n    <name>hadoop.http.staticuser.user</name>\n    <value>bigdata</value>\n  </property>\n \n \x3c!-- \u56de\u6536\u7ad9 --\x3e\n    \x3c!-- \u8868\u793a\u56de\u6536\u7ad9\u6587\u4ef6\u5728\u5220\u9664\u4e4b\u524d\u5c06\u88ab\u4fdd\u7559 3600 \u79d2 --\x3e\n    <property>\n        <name>fs.trash.interval</name>\n        <value>3600</value>\n    </property>\n \n \x3c!-- \u751f\u6210 Trash \u68c0\u67e5\u70b9\u7684\u65f6\u95f4\u95f4\u9694\u3002\u68c0\u67e5\u70b9\u5305\u542b\u7528\u6237\u5220\u9664\u7684\u6587\u4ef6\u548c\u76ee\u5f55\u7684\u5feb\u7167\u3002 --\x3e\n    <property>\n        <name>fs.trash.checkpoint.interval</name>\n        <value>3600</value>\n    </property>\n \n    \x3c!-- \u914d\u7f6e\u8be5bigdata(superUser)\u5141\u8bb8\u901a\u8fc7\u4ee3\u7406\u8bbf\u95ee\u7684\u4e3b\u673a\u8282\u70b9 --\x3e\n    <property>\n        <name>hadoop.proxyuser.bigdata.hosts</name>\n        <value>*</value>\n    </property>\n    \x3c!-- \u914d\u7f6e\u8be5bigdata(superUser)\u5141\u8bb8\u901a\u8fc7\u4ee3\u7406\u7528\u6237\u6240\u5c5e\u7ec4 --\x3e\n    <property>\n        <name>hadoop.proxyuser.bigdata.groups</name>\n        <value>*</value>\n    </property>\n    \x3c!-- \u914d\u7f6e\u8be5bigdata(superUser)\u5141\u8bb8\u901a\u8fc7\u4ee3\u7406\u7684\u7528\u6237--\x3e\n    <property>\n        <name>hadoop.proxyuser.bigdata.users</name>\n        <value>*</value>\n    </property>\n \n \x3c!-- \u6307\u5b9azkfc\u8981\u8fde\u63a5\u7684zkServer\u5730\u5740 --\x3e\n<property>\n    <name>ha.zookeeper.quorum</name>\n    <value>master1:2181,master2:2181,node1:2181</value>\n</property>\n\n\x3c!-- Hue  --\x3e\n<property>\n    <name>hadoop.proxyuser.hdfs.hosts</name>\n    <value>*</value>\n</property>\n<property>\n    <name>hadoop.proxyuser.hdfs.groups</name>\n    <value>*</value>\n</property>\n\n<property>\n  <name>hadoop.proxyuser.httpfs.hosts</name>\n  <value>*</value>\n</property>\n<property>\n  <name>hadoop.proxyuser.httpfs.groups</name>\n  <value>*</value>\n</property>\n\n<property>\n        <name>hadoop.proxyuser.hue.hosts</name>\n        <value>*</value>\n    </property>\n    <property>\n        <name>hadoop.proxyuser.hue.groups</name>\n        <value>*</value>\n    </property>\n\n</configuration>\n\n')),(0,t.kt)("h3",{id:"hdfs-sitexml"},"hdfs-site.xml"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"vi /home/bigdata/module/hadoop-3.2.3/etc/hadoop/hdfs-site.xml\n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"touch /home/bigdata/module/hadoop-3.2.3/etc/blacklist \n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},'<?xml version="1.0" encoding="UTF-8"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n\x3c!--\n  Licensed under the Apache License, Version 2.0 (the "License");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an "AS IS" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n--\x3e\n\n\x3c!-- Put site-specific property overrides in this file. --\x3e\n\n<configuration>\n\n\x3c!-- NameNode\u6570\u636e\u5b58\u50a8\u76ee\u5f55 --\x3e\n  <property>\n    <name>dfs.namenode.name.dir</name>\n    <value>file://${hadoop.tmp.dir}/name</value>\n  </property>\n\x3c!-- DataNode\u6570\u636e\u5b58\u50a8\u76ee\u5f55 --\x3e\n  <property>\n    <name>dfs.datanode.data.dir</name>\n    <value>file://${hadoop.tmp.dir}/data</value>\n  </property>\n\x3c!-- JournalNode\u6570\u636e\u5b58\u50a8\u76ee\u5f55 --\x3e\n  <property>\n    <name>dfs.journalnode.edits.dir</name>\n    <value>${hadoop.tmp.dir}/jn</value>\n  </property>\n\x3c!-- \u5b8c\u5168\u5206\u5e03\u5f0f\u96c6\u7fa4\u540d\u79f0 \u5bf9\u5e94core.xml\u91cc\u9762\u7684fs.defaultFS--\x3e\n  <property>\n    <name>dfs.nameservices</name>\n    <value>bigdatacluster</value>\n  </property>\n\x3c!-- \u96c6\u7fa4\u4e2dNameNode\u8282\u70b9\u90fd\u6709\u54ea\u4e9b --\x3e\n  <property>\n    <name>dfs.ha.namenodes.bigdatacluster</name>\n    <value>nn1,nn2</value>\n  </property>\n\x3c!-- NameNode\u7684RPC\u901a\u4fe1\u5730\u5740 --\x3e\n  <property>\n    <name>dfs.namenode.rpc-address.bigdatacluster.nn1</name>\n    <value>master1:8020</value>\n  </property>\n  <property>\n    <name>dfs.namenode.rpc-address.bigdatacluster.nn2</name>\n    <value>master2:8020</value>\n  </property>\n\x3c!-- NameNode\u7684http\u901a\u4fe1\u5730\u5740 --\x3e\n  <property>\n    <name>dfs.namenode.http-address.bigdatacluster.nn1</name>\n    <value>master1:9870</value>\n  </property>\n  <property>\n    <name>dfs.namenode.http-address.bigdatacluster.nn2</name>\n    <value>master2:9870</value>\n  </property>\n\x3c!-- \u6307\u5b9aNameNode\u5143\u6570\u636e\u5728JournalNode\u4e0a\u7684\u5b58\u653e\u4f4d\u7f6e --\x3e\n  <property>\n<name>dfs.namenode.shared.edits.dir</name>\n<value>qjournal://master1:8485;master2:8485;node1:8485/bigdatacluster</value>\n  </property>\n\x3c!-- \u8bbf\u95ee\u4ee3\u7406\u7c7b\uff1aclient\u7528\u4e8e\u786e\u5b9a\u54ea\u4e2aNameNode\u4e3aActive --\x3e\n  <property>\n    <name>dfs.client.failover.proxy.provider.bigdatacluster</name>\n    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n  </property>\n\x3c!-- \u914d\u7f6e\u9694\u79bb\u673a\u5236\uff0c\u5373\u540c\u4e00\u65f6\u523b\u53ea\u80fd\u6709\u4e00\u53f0\u670d\u52a1\u5668\u5bf9\u5916\u54cd\u5e94 --\x3e\n  <property>\n    <name>dfs.ha.fencing.methods</name>\n    <value>sshfence</value>\n  </property>\n\x3c!-- \u4f7f\u7528\u9694\u79bb\u673a\u5236\u65f6\u9700\u8981ssh\u79d8\u94a5\u767b\u5f55--\x3e\n  <property>\n    <name>dfs.ha.fencing.ssh.private-key-files</name>\n    <value>/home/bigdata/.ssh/id_rsa</value>\n  </property>\n \n  \x3c!-- \u914d\u7f6e\u9ed1\u540d\u5355 --\x3e\n     <property>\n        <name>dfs.hosts.exclude</name>\n        <value>/home/bigdata/module/hadoop-3.1.3/etc/blacklist</value>\n    </property>\n    \n    \x3c!-- \u542f\u7528nn\u6545\u969c\u81ea\u52a8\u8f6c\u79fb --\x3e\n<property>\n    <name>dfs.ha.automatic-failover.enabled</name>\n    <value>true</value>\n</property>\n\n\x3c!-- HUE --\x3e\n<property>\n    <name>dfs.webhdfs.enabled</name>\n    <value>true</value>\n</property>\n<property>\n    <name>dfs.permissions.enabled</name>\n    <value>false</value>\n</property>\n\n\x3c!-- \u78c1\u76d8\u6269\u5bb9\u7b56\u7565 --\x3e\n\x3c!-- \u8bbe\u7f6e\u6570\u636e\u5b58\u50a8\u7b56\u7565\uff0c\u9ed8\u8ba4\u4e3a\u8f6e\u8be2\uff0c\u73b0\u5728\u7684\u60c5\u51b5\u663e\u7136\u5e94\u8be5\u7528\u201c\u9009\u62e9\u7a7a\u95f4\u591a\u7684\u78c1\u76d8\u5b58\u201d\u6a21\u5f0f --\x3e\n<property>\n    <name>dfs.datanode.fsdataset.volume.choosing.policy</name>\n    <value>org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy</value>\n</property>\n\x3c!-- \u9ed8\u8ba4\u503c0.75\u3002\u5b83\u7684\u542b\u4e49\u662f\u6570\u636e\u5757\u5b58\u50a8\u5230\u53ef\u7528\u7a7a\u95f4\u591a\u7684\u5377\u4e0a\u7684\u6982\u7387\uff0c\u7531\u6b64\u53ef\u89c1\uff0c\u8fd9\u4e2a\u503c\u5982\u679c\u53d60.5\u4ee5\u4e0b\uff0c\u5bf9\u8be5\u7b56\u7565\u800c\u8a00\u662f\u6beb\u65e0\u610f\u4e49\u7684\uff0c\u4e00\u822c\u5c31\u91c7\u7528\u9ed8\u8ba4\u503c\u3002--\x3e\n<property>\n    <name>dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction</name>\n    <value>0.9f</value>\n</property>\n\x3c!-- \u914d\u7f6e\u5404\u4e2a\u78c1\u76d8\u7684\u5747\u8861\u9608\u503c\u7684\uff0c\u9ed8\u8ba4\u4e3a10G\uff0810737418240\uff09\uff0c\u5728\u6b64\u8282\u70b9\u7684\u6240\u6709\u6570\u636e\u5b58\u50a8\u7684\u76ee\u5f55\u4e2d\uff0c\u627e\u4e00\u4e2a\u5360\u7528\u6700\u5927\u7684\uff0c\u627e\u4e00\u4e2a\u5360\u7528\u6700\u5c0f\u7684\uff0c\u5982\u679c\u5728\u4e24\u8005\u4e4b\u5dee\u572810G\u7684\u8303\u56f4\u5185\uff0c\u90a3\u4e48\u5757\u5206\u914d\u7684\u65b9\u5f0f\u662f\u8f6e\u8be2\u3002 --\x3e\n<property>\n  <name>dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold</name>         \n  <value>10737418240</value>\n</property>\n\n<property>\n  <name>dfs.disk.balancer.enabled</name>\n  <value>true</value>\n</property>\n</configuration>\n\n')),(0,t.kt)("h3",{id:"yarn-sitexml"},"yarn-site.xml"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"vi /home/bigdata/module/hadoop-3.2.3/etc/hadoop/yarn-site.xml\n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},'<?xml version="1.0"?>\n\x3c!--\n  Licensed under the Apache License, Version 2.0 (the "License");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an "AS IS" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n--\x3e\n\n<configuration>\n \n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n \n    \x3c!-- \u542f\u7528resourcemanager ha --\x3e\n    <property>\n        <name>yarn.resourcemanager.ha.enabled</name>\n        <value>true</value>\n    </property>\n \n    \x3c!-- \u58f0\u660e\u4e24\u53f0resourcemanager\u7684\u5730\u5740 --\x3e\n    <property>\n        <name>yarn.resourcemanager.cluster-id</name>\n        <value>cluster-yarn1</value>\n    </property>\n    \x3c!--\u6307\u5b9aresourcemanager\u7684\u903b\u8f91\u5217\u8868--\x3e\n    <property>\n        <name>yarn.resourcemanager.ha.rm-ids</name>\n        <value>rm1,rm2</value>\n</property>\n\x3c!-- ========== rm1\u7684\u914d\u7f6e ========== --\x3e\n\x3c!-- \u6307\u5b9arm1\u7684\u4e3b\u673a\u540d --\x3e\n    <property>\n        <name>yarn.resourcemanager.hostname.rm1</name>\n        <value>master1</value>\n</property>\n\x3c!-- \u6307\u5b9arm1\u7684web\u7aef\u5730\u5740 --\x3e\n<property>\n     <name>yarn.resourcemanager.webapp.address.rm1</name>\n     <value>master1:8088</value>\n</property>\n\x3c!-- \u6307\u5b9arm1\u7684\u5185\u90e8\u901a\u4fe1\u5730\u5740 --\x3e\n<property>\n     <name>yarn.resourcemanager.address.rm1</name>\n     <value>master1:8032</value>\n</property>\n\x3c!-- \u6307\u5b9aAM\u5411rm1\u7533\u8bf7\u8d44\u6e90\u7684\u5730\u5740 --\x3e\n<property>\n     <name>yarn.resourcemanager.scheduler.address.rm1</name>  \n     <value>master1:8030</value>\n</property>\n\x3c!-- \u6307\u5b9a\u4f9bNM\u8fde\u63a5\u7684\u5730\u5740 --\x3e  \n<property>\n     <name>yarn.resourcemanager.resource-tracker.address.rm1</name>\n     <value>master1:8031</value>\n</property>\n\x3c!-- ========== rm2\u7684\u914d\u7f6e ========== --\x3e\n    \x3c!-- \u6307\u5b9arm2\u7684\u4e3b\u673a\u540d --\x3e\n    <property>\n        <name>yarn.resourcemanager.hostname.rm2</name>\n        <value>master2</value>\n</property>\n<property>\n     <name>yarn.resourcemanager.webapp.address.rm2</name>\n     <value>master2:8088</value>\n</property>\n<property>\n     <name>yarn.resourcemanager.address.rm2</name>\n     <value>master2:8032</value>\n</property>\n<property>\n     <name>yarn.resourcemanager.scheduler.address.rm2</name>\n     <value>master2:8030</value>\n</property>\n<property>\n     <name>yarn.resourcemanager.resource-tracker.address.rm2</name>\n     <value>master2:8031</value>\n</property>\n \n    \x3c!-- \u6307\u5b9azookeeper\u96c6\u7fa4\u7684\u5730\u5740 --\x3e \n    <property>\n        <name>yarn.resourcemanager.zk-address</name>\n        <value>master1:2181,master2:2181,node1:2181</value>\n    </property>\n \n    \x3c!-- \u542f\u7528\u81ea\u52a8\u6062\u590d --\x3e \n    <property>\n        <name>yarn.resourcemanager.recovery.enabled</name>\n        <value>true</value>\n    </property>\n \n    \x3c!-- \u6307\u5b9aresourcemanager\u7684\u72b6\u6001\u4fe1\u606f\u5b58\u50a8\u5728zookeeper\u96c6\u7fa4 --\x3e \n    <property>\n        <name>yarn.resourcemanager.store.class</name>     \n        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>\n</property>\n\x3c!-- \u73af\u5883\u53d8\u91cf\u7684\u7ee7\u627f --\x3e\n <property>\n        <name>yarn.nodemanager.env-whitelist</name>\n        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>\n    </property>\n \n     \x3c!-- \u5f00\u542f\u65e5\u5fd7\u805a\u96c6\u529f\u80fd --\x3e\n  <property>\n    <name>yarn.log-aggregation-enable</name>\n    <value>true</value>\n  </property>\n  \x3c!-- \u8bbe\u7f6e\u65e5\u5fd7\u805a\u96c6\u670d\u52a1\u5668\u5730\u5740 --\x3e\n \x3c!-- \u8bbe\u7f6e\u65e5\u5fd7\u805a\u96c6\u670d\u52a1\u5668\u5730\u5740 --\x3e\n  <property>  \n    <name>yarn.log.server.url</name>  \n    <value>http://master1:19888/jobhistory/logs</value>\n  </property>\n  \x3c!-- \u8bbe\u7f6e\u65e5\u5fd7\u4fdd\u7559\u65f6\u95f4\u4e3a7\u5929 --\x3e\n  <property>\n    <name>yarn.log-aggregation.retain-seconds</name>\n    <value>604800</value>\n  </property>\n \n    \x3c!--\u662f\u5426\u542f\u52a8\u4e00\u4e2a\u7ebf\u7a0b\u68c0\u67e5\u6bcf\u4e2a\u4efb\u52a1\u6b63\u4f7f\u7528\u7684\u7269\u7406\u5185\u5b58\u91cf\uff0c\u5982\u679c\u4efb\u52a1\u8d85\u51fa\u5206\u914d\u503c\uff0c\u5219\u76f4\u63a5\u5c06\u5176\u6740\u6389\uff0c\u9ed8\u8ba4\u662ftrue --\x3e\n    <property>\n        <name>yarn.nodemanager.pmem-check-enabled</name>\n        <value>false</value>\n    </property>\n \n    \x3c!--\u662f\u5426\u542f\u52a8\u4e00\u4e2a\u7ebf\u7a0b\u68c0\u67e5\u6bcf\u4e2a\u4efb\u52a1\u6b63\u4f7f\u7528\u7684\u865a\u62df\u5185\u5b58\u91cf\uff0c\u5982\u679c\u4efb\u52a1\u8d85\u51fa\u5206\u914d\u503c\uff0c\u5219\u76f4\u63a5\u5c06\u5176\u6740\u6389\uff0c\u9ed8\u8ba4\u662ftrue --\x3e\n    <property>\n        <name>yarn.nodemanager.vmem-check-enabled</name>\n        <value>false</value>\n    </property>\n\n\x3c!--nodemanager \u7684\u5185\u5b58--\x3e\n    <property>\n      <name>yarn.nodemanager.resource.memory-mb</name>\n      <value>51200</value>\n    </property>\n\x3c!--\u7f3a\u5c11\u8be5\u9879\u5bb9\u6613\u5f15\u53d1\u5f02\u5e38\uff1aInvalid event: APP_UPDATE_SAVED at ACCEPTED--\x3e\n    <property>\n        <name>yarn.app.mapreduce.am.resource.mb</name>\n        <value>2048</value>\n    </property>\n    <property>\n    \x3c!--\u8868\u793a\u8be5\u8282\u70b9\u670d\u52a1\u5668\u4e0ayarn\u53ef\u4ee5\u4f7f\u7528\u7684\u865a\u62dfCPU\u4e2a\u6570\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u4e3a\u5b9e\u9645\u673a\u5668\u7684\u4e24\u500d--\x3e\n      <name>yarn.nodemanager.resource.cpu-vcores</name>\n      <value>28</value>\n    </property>\n\n</configuration>\n\n')),(0,t.kt)("h3",{id:"mapred-sitexml"},"mapred-site.xml"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"vi /home/bigdata/module/hadoop-3.2.3/etc/hadoop/mapred-site.xml\n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},'<?xml version="1.0"?>\n<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n\x3c!--\n  Licensed under the Apache License, Version 2.0 (the "License");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an "AS IS" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n--\x3e\n\n\x3c!-- Put site-specific property overrides in this file. --\x3e\n\n<configuration>\n\n\x3c!-- \u542f\u7528jvm\u91cd\u7528 --\x3e\n<property>\n    <name>mapreduce.job.jvm.numtasks</name>\n    <value>10</value>\n    <description>How many tasks to run per jvm,if set to -1 ,there is  no limit</description>\n</property>\n\n\x3c!--    \n <property>\n      <name>mapreduce.job.tracker</name>\n      <value>hdfs://master1:8001</value>\n      <final>true</final>\n </property>\n--\x3e\n<property>\n        \x3c!--\u6307\u5b9a mapreduce \u4f5c\u4e1a\u8fd0\u884c\u5728 yarn \u4e0a--\x3e\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n    </property>\n    \n<property>\n  <name>yarn.app.mapreduce.am.env</name>\n  <value>HADOOP_MAPRED_HOME=/home/bigdata/module/hadoop-3.2.3</value>\n</property>\n<property>\n  <name>mapreduce.map.env</name>\n  <value>HADOOP_MAPRED_HOME=/home/bigdata/module/hadoop-3.2.3</value>\n</property>\n<property>\n  <name>mapreduce.reduce.env</name>\n  <value>HADOOP_MAPRED_HOME=/home/bigdata/module/hadoop-3.2.3</value>\n</property>\n \n\x3c!-- \u5386\u53f2\u670d\u52a1\u5668\u7aef\u5730\u5740 --\x3e\n  <property>\n    <name>mapreduce.jobhistory.address</name>\n    <value>master1:10020</value>\n  </property>\n  \x3c!-- \u5386\u53f2\u670d\u52a1\u5668web\u7aef\u5730\u5740 --\x3e\n  <property>\n    <name>mapreduce.jobhistory.webapp.address</name>\n    <value>master1:19888</value>\n  </property>\n\n</configuration>\n\n')),(0,t.kt)("h3",{id:"capacity-schedulerxml"},"capacity-scheduler.xml"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"vi /home/bigdata/module/hadoop-3.2.3/etc/hadoop/capacity-scheduler.xml\n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},'\x3c!--\n       Licensed under the Apache License, Version 2.0 (the "License");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an "AS IS" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n--\x3e\n<configuration>\n\n<property>\n    <name>yarn.scheduler.capacity.maximum-applications</name>\n    <value>10000</value>\n    <description>\n      Maximum number of applications that can be pending and running.\n    </description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>\n    <value>0.6</value>\n    <description>\n      Maximum percent of resources in the cluster which can be used to run \n      application masters i.e. controls number of concurrent running\n      applications.\n    </description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.resource-calculator</name>\n    <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>\n    <description>\n      The ResourceCalculator implementation to be used to compare \n      Resources in the scheduler.\n      The default i.e. DefaultResourceCalculator only uses Memory while\n      DominantResourceCalculator uses dominant-resource to compare \n      multi-dimensional resources such as Memory, CPU etc.\n    </description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.root.queues</name>\n    <value>high,default</value>\n    <description>\n      The queues at the this level (root is the root queue).\n    </description>\n  </property>\n\x3c!--\n            \u961f\u5217\u5360\u6bd4\n--\x3e\n  <property>\n    <name>yarn.scheduler.capacity.root.high.capacity</name>\n    <value>60</value>\n    <description>Default queue target capacity.</description>\n  </property>\n\n  \n   <property>\n    <name>yarn.scheduler.capacity.root.default.capacity</name>\n    <value>40</value>\n    <description>Default queue target capacity.</description>\n  </property>\n\n  \n\x3c!--\n            \u767e\u5206\u6bd4\n--\x3e\n  <property>\n    <name>yarn.scheduler.capacity.root.high.user-limit-factor</name>\n    <value>1</value>\n    <description>\n      Default queue user limit a percentage from 0.0 to 1.0.\n    </description>\n  </property>\n\n\n    <property>\n    <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>\n    <value>1</value>\n    <description>\n      Default queue user limit a percentage from 0.0 to 1.0.\n    </description>\n  </property>\n\n\x3c!--\n            \u8fd0\u884c\u72b6\u6001\n--\x3e\n  <property>\n    <name>yarn.scheduler.capacity.root.high.maximum-capacity</name>\n    <value>100</value>\n    <description>\n      The maximum capacity of the default queue. \n    </description>\n  </property>\n  \n    <property>\n    <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>\n    <value>100</value>\n    <description>\n      The maximum capacity of the default queue. \n    </description>\n  </property>\n\n\n   <property>\n    <name>yarn.scheduler.capacity.root.high.state</name>\n    <value>RUNNING</value>\n    <description>\n      The state of the default queue. State can be one of RUNNING or STOPPED.\n    </description>\n  </property>\n  \n  \n     <property>\n    <name>yarn.scheduler.capacity.root.default.state</name>\n    <value>RUNNING</value>\n    <description>\n      The state of the default queue. State can be one of RUNNING or STOPPED.\n    </description>\n  </property>\n\n\x3c!--\n            \u6743\u9650\n--\x3e\n  <property>\n    <name>yarn.scheduler.capacity.root.high.acl_submit_applications</name>\n    <value>*</value>\n    <description>\n      The ACL of who can submit jobs to the default queue.\n    </description>\n  </property>\n\n\n    <property>\n    <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>\n    <value>*</value>\n    <description>\n      The ACL of who can submit jobs to the default queue.\n    </description>\n  </property>\n\x3c!--\n            \u6743\u9650\n--\x3e\n  <property>\n    <name>yarn.scheduler.capacity.root.high.acl_administer_queue</name>\n    <value>*</value>\n    <description>\n      The ACL of who can administer jobs on the default queue.\n    </description>\n  </property>\n\n  \n      <property>\n    <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>\n    <value>*</value>\n    <description>\n      The ACL of who can administer jobs on the default queue.\n    </description>\n  </property>\n\n\x3c!--\n            \u6743\u9650\n--\x3e\n  <property>\n    <name>yarn.scheduler.capacity.root.high.acl_application_max_priority</name>\n    <value>*</value>\n    <description>\n      The ACL of who can submit applications with configured priority.\n      For e.g, [user={name} group={name} max_priority={priority} default_priority={priority}]\n    </description>\n  </property>\n\n  \n      <property>\n    <name>yarn.scheduler.capacity.root.default.acl_application_max_priority</name>\n    <value>*</value>\n    <description>\n      The ACL of who can submit applications with configured priority.\n      For e.g, [user={name} group={name} max_priority={priority} default_priority={priority}]\n    </description>\n  </property>\n\n\x3c!--\n            \u6743\u9650\n--\x3e\n   <property>\n     <name>yarn.scheduler.capacity.root.high.maximum-application-lifetime\n     </name>\n     <value>-1</value>\n     <description>\n        Maximum lifetime of an application which is submitted to a queue\n        in seconds. Any value less than or equal to zero will be considered as\n        disabled.\n        This will be a hard time limit for all applications in this\n        queue. If positive value is configured then any application submitted\n        to this queue will be killed after exceeds the configured lifetime.\n        User can also specify lifetime per application basis in\n        application submission context. But user lifetime will be\n        overridden if it exceeds queue maximum lifetime. It is point-in-time\n        configuration.\n        Note : Configuring too low value will result in killing application\n        sooner. This feature is applicable only for leaf queue.\n     </description>\n   </property>\n\n\n\n      <property>\n     <name>yarn.scheduler.capacity.root.default.maximum-application-lifetime\n     </name>\n     <value>-1</value>\n     <description>\n        Maximum lifetime of an application which is submitted to a queue\n        in seconds. Any value less than or equal to zero will be considered as\n        disabled.\n        This will be a hard time limit for all applications in this\n        queue. If positive value is configured then any application submitted\n        to this queue will be killed after exceeds the configured lifetime.\n        User can also specify lifetime per application basis in\n        application submission context. But user lifetime will be\n        overridden if it exceeds queue maximum lifetime. It is point-in-time\n        configuration.\n        Note : Configuring too low value will result in killing application\n        sooner. This feature is applicable only for leaf queue.\n     </description>\n   </property>\n\n\x3c!--\n            \u751f\u547d\u5468\u671f\n--\x3e\n   <property>\n     <name>yarn.scheduler.capacity.root.high.default-application-lifetime\n     </name>\n     <value>-1</value>\n     <description>\n        Default lifetime of an application which is submitted to a queue\n        in seconds. Any value less than or equal to zero will be considered as\n        disabled.\n        If the user has not submitted application with lifetime value then this\n        value will be taken. It is point-in-time configuration.\n        Note : Default lifetime can\'t exceed maximum lifetime. This feature is\n        applicable only for leaf queue.\n     </description>\n   </property>\n   \n   <property>\n        <name>yarn.scheduler.capacity.root.default.default-application-lifetime\n     </name>\n     <value>-1</value>\n     <description>\n        Default lifetime of an application which is submitted to a queue\n        in seconds. Any value less than or equal to zero will be considered as\n        disabled.\n        If the user has not submitted application with lifetime value then this\n        value will be taken. It is point-in-time configuration.\n        Note : Default lifetime can\'t exceed maximum lifetime. This feature is\n        applicable only for leaf queue.\n     </description>\n   </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.node-locality-delay</name>\n    <value>40</value>\n    <description>\n      Number of missed scheduling opportunities after which the CapacityScheduler \n      attempts to schedule rack-local containers.\n      When setting this parameter, the size of the cluster should be taken into account.\n      We use 40 as the default value, which is approximately the number of nodes in one rack.\n      Note, if this value is -1, the locality constraint in the container request\n      will be ignored, which disables the delay scheduling.\n    </description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.rack-locality-additional-delay</name>\n    <value>-1</value>\n    <description>\n      Number of additional missed scheduling opportunities over the node-locality-delay\n      ones, after which the CapacityScheduler attempts to schedule off-switch containers,\n      instead of rack-local ones.\n      Example: with node-locality-delay=40 and rack-locality-delay=20, the scheduler will\n      attempt rack-local assignments after 40 missed opportunities, and off-switch assignments\n      after 40+20=60 missed opportunities.\n      When setting this parameter, the size of the cluster should be taken into account.\n      We use -1 as the default value, which disables this feature. In this case, the number\n      of missed opportunities for assigning off-switch containers is calculated based on\n      the number of containers and unique locations specified in the resource request,\n      as well as the size of the cluster.\n    </description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.queue-mappings</name>\n    <value></value>\n    <description>\n      A list of mappings that will be used to assign jobs to queues\n      The syntax for this list is [u|g]:[name]:[queue_name][,next mapping]*\n      Typically this list will be used to map users to queues,\n      for example, u:%user:%user maps all users to queues with the same name\n      as the user.\n    </description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.queue-mappings-override.enable</name>\n    <value>false</value>\n    <description>\n      If a queue mapping is present, will it override the value specified\n      by the user? This can be used by administrators to place jobs in queues\n      that are different than the one specified by the user.\n      The default is false.\n    </description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.per-node-heartbeat.maximum-offswitch-assignments</name>\n    <value>1</value>\n    <description>\n      Controls the number of OFF_SWITCH assignments allowed\n      during a node\'s heartbeat. Increasing this value can improve\n      scheduling rate for OFF_SWITCH containers. Lower values reduce\n      "clumping" of applications on particular nodes. The default is 1.\n      Legal values are 1-MAX_INT. This config is refreshable.\n    </description>\n  </property>\n\n\n  <property>\n    <name>yarn.scheduler.capacity.application.fail-fast</name>\n    <value>false</value>\n    <description>\n      Whether RM should fail during recovery if previous applications\'\n      queue is no longer valid.\n    </description>\n  </property>\n\n</configuration>\n\n')),(0,t.kt)("h3",{id:"hadoop-envsh"},"hadoop-env.sh"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"mkdir /home/bigdata/module/hadoop-3.2.3/pid\n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"vi /home/bigdata/module/hadoop-3.2.3/etc/hadoop/hadoop-env.sh\n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"export HDFS_NAMENODE_USER=bigdata\nexport HADOOP_PID_DIR=/home/bigdata/module/hadoop-3.2.3/pid\n")),(0,t.kt)("h3",{id:"\u540c\u6b65\u914d\u7f6e\u6587\u4ef6"},"\u540c\u6b65\u914d\u7f6e\u6587\u4ef6"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"/home/bigdata/shell/xsync.sh /home/bigdata/module/hadoop-3.2.3/etc/hadoop\n")),(0,t.kt)("h2",{id:"\u542f\u52a8\u96c6\u7fa4"},"\u542f\u52a8\u96c6\u7fa4"),(0,t.kt)("h3",{id:"\u542f\u52a8\u51c6\u5907"},"\u542f\u52a8\u51c6\u5907"),(0,t.kt)("ul",null,(0,t.kt)("li",{parentName:"ul"},"\u542f\u52a8Zookeeper\u4ee5\u540e\uff0c\u7136\u540e\u518d\u521d\u59cb\u5316HA\u5728Zookeeper\u4e2d\u72b6\u6001(\u5728\u5bf9\u5e94\u7684namenode\u4e0a\u9762\u5176\u4e2d\u4e00\u53f0\u683c\u5f0f\u5316\u5c31\u884c\uff0c\u6700\u597d\u8fd9\u79cd\u60c5\u51b5\u662fmaster1\u4e3b\u8282\u70b9)\u3002")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"hdfs zkfc -formatZK\n")),(0,t.kt)("ul",null,(0,t.kt)("li",{parentName:"ul"},"\u542f\u52a8journalnode\u4ed6\u662f8485\u7aef\u53e3(\u5728datanode\u7684\u6240\u6709\u673a\u5668\u4e0a\u9762\u542f\u52a8)\u3002")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"hdfs --daemon start journalnode\n")),(0,t.kt)("ul",null,(0,t.kt)("li",{parentName:"ul"},"\u7136\u540e\u683c\u5f0f\u5316\uff08namenode\uff1a\u4e3b\u8282\u70b9\u683c\u5f0f\u5316\uff0c\u8fd9\u91cc\u4e5f\u5c31\u662fmaster1\uff09\u3002")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"hdfs namenode -format\n")),(0,t.kt)("ul",null,(0,t.kt)("li",{parentName:"ul"},"master1\u542f\u52a8namenode (master2\u5148\u4e0d\u7528\u542f\u52a8)\u3002")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"hdfs --daemon start namenode\n")),(0,t.kt)("ul",null,(0,t.kt)("li",{parentName:"ul"},"master2\u7136\u540e\u540c\u6b65namenode\u6570\u636e\u3002")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"hdfs namenode -bootstrapStandby \n")),(0,t.kt)("blockquote",null,(0,t.kt)("p",{parentName:"blockquote"},'\u8fd0\u884c\u96c6\u7fa4\u7684\u65f6\u5019\u6ce8\u610f\uff1a\u5982\u679c\u6709JAVA_HOME\u627e\u4e0d\u5230\uff0c\u90a3\u4e48\u5c31\u628ahadoop\u91cc\u9762\u7684bin\u548csbin\u91cc\u9762\u7684\u6267\u884c\u811a\u672c\u90fd\u52a0\u4e00\u4e2aJAVA_HOME\u53d8\u91cf\uff0c\u6bd4\u5982\uff1aJAVA_HOME="/home/bigdata/module/jdk1.8.0_161"\u52a0\u5165\u5230\u811a\u672c\u9760\u524d\u7684\u4f4d\u7f6e\u5c31\u884c\u4e86\u3002')),(0,t.kt)("h3",{id:"\u96c6\u7fa4\u542f\u52a8\u811a\u672c"},"\u96c6\u7fa4\u542f\u52a8\u811a\u672c"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"vi /home/bigdata/shell/hadoop_server.sh\n\nchmod 777 /home/bigdata/shell/hadoop_server.sh\n\n/home/bigdata/shell/hadoop_server.sh start\n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},'#!/bin/bash\nif [ $# -lt 1 ]\nthen\n    echo "No Args Input..."\n    exit ;\nfi\ncase $1 in\n"start")\n        echo " =================== \u542f\u52a8 hadoop\u96c6\u7fa4 ==================="\n        echo " master1 \u7684journalnode\u542f\u52a8"\n        ssh master1 "hdfs --daemon start journalnode"\n        echo " master2 \u7684journalnode\u542f\u52a8"\n        ssh master2 "hdfs --daemon start journalnode"\n        echo " node1 \u7684journalnode\u542f\u52a8"\n        ssh node1 "hdfs --daemon start journalnode"\n        \n        \n        echo " --------------- \u542f\u52a8 hdfs ---------------"\n        ssh master1 "/home/bigdata/module/hadoop-3.2.3/sbin/start-dfs.sh"\n        echo " --------------- \u542f\u52a8 yarn ---------------"\n        ssh master2 "/home/bigdata/module/hadoop-3.2.3/sbin/start-yarn.sh"\n        \n        echo " --------------- \u542f\u52a8 historyserver ---------------"\n        ssh master1 "/home/bigdata/module/hadoop-3.2.3/bin/mapred --daemon start historyserver"  \n        echo " --------------- \u542f\u52a8 httpfs ---------------"\n        ssh master1 "/home/bigdata/module/hadoop-3.2.3/bin/hdfs --daemon start httpfs"\n;;\n"stop")\n        echo " --------------- \u5173\u95edhttpfs ---------------"\n        ssh master1 "/home/bigdata/module/hadoop-3.2.3/bin/hdfs --daemon stop httpfs"\n        echo " =================== \u5173\u95ed hadoop\u96c6\u7fa4 ==================="\n        echo " --------------- \u5173\u95ed historyserver ---------------"\n        ssh master1 "/home/bigdata/module/hadoop-3.2.3/bin/mapred --daemon stop historyserver"\n        \n        echo " --------------- \u5173\u95ed yarn ---------------"\n        ssh master2 "/home/bigdata/module/hadoop-3.2.3/sbin/stop-yarn.sh"\n        echo " --------------- \u5173\u95ed hdfs ---------------"\n        ssh master1 "/home/bigdata/module/hadoop-3.2.3/sbin/stop-dfs.sh"\n        \n        echo " master1 \u7684journalnode\u5173\u95ed"\n        ssh master1 "hdfs --daemon stop journalnode"\n        echo " master2 \u7684journalnode\u5173\u95ed"\n        ssh master2 "hdfs --daemon stop journalnode"\n        echo " node1 \u7684journalnode\u5173\u95ed"\n        ssh node1 "hdfs --daemon stop journalnode"\n;;\n*)\n    echo "Input Args Error..."\n;;\nesac\n\n')),(0,t.kt)("h2",{id:"\u6d4b\u8bd5\u770b\u662f\u5426\u542f\u52a8\u6210\u529f"},"\u6d4b\u8bd5\u770b\u662f\u5426\u542f\u52a8\u6210\u529f"),(0,t.kt)("h3",{id:"\u67e5\u770bhdfs\u7684\u9ad8\u53ef\u7528\u72b6\u6001"},"\u67e5\u770bhdfs\u7684\u9ad8\u53ef\u7528\u72b6\u6001"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"hdfs haadmin -getAllServiceState\n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"yarn rmadmin -getAllServiceState\n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"[bigdata@master1 shell]$ hdfs haadmin -getAllServiceState\nmaster1:8020                                       standby   \nmaster2:8020                                       active  \n\n[bigdata@master1 shell]$ yarn rmadmin -getAllServiceState\nmaster1:8033                                       standby   \nmaster2:8033                                       active  \n")),(0,t.kt)("ul",null,(0,t.kt)("li",{parentName:"ul"},"\u6d4b\u8bd5resourcemanger\u9ad8\u53ef\u7528\u662f\u5426\u53ef\u7528\u3002")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"/home/bigdata/module/hadoop-3.2.3/sbin/yarn-daemon.sh start resourcemanager\n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"/home/bigdata/module/hadoop-3.2.3/sbin/yarn-daemon.sh stop resourcemanager\n")),(0,t.kt)("ul",null,(0,t.kt)("li",{parentName:"ul"},"\u6d4b\u8bd5namenode\u9ad8\u53ef\u7528\u662f\u5426\u53ef\u7528\u3002")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"hdfs --daemon start namenode\n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"hdfs --daemon stop namenode\n")),(0,t.kt)("h3",{id:"\u63d0\u4ea4\u4e00\u4e2a-mr-\u7a0b\u5e8f"},"\u63d0\u4ea4\u4e00\u4e2a mr \u7a0b\u5e8f\u3002"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"hadoop dfs -mkdir /data\n\nhadoop dfs -put /home/bigdata/shell/hadoop_server.sh /data\n")),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},"hadoop jar /home/bigdata/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar wordcount  -Dmapreduce.job.queuename=default /data /output\n")))}c.isMDXComponent=!0}}]);